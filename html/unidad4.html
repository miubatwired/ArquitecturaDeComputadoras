<!-- -->
<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title> Unidad 4</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="../css/Style.css" media="screen">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="preload" href="/media/img/Robo-Advisors2.jpg" as="image" fetchpriority="high">
        <link rel="prefetch" href="/media/img/Robo-Advisors.jpg" as="image" fetchpriority="high">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Fira+Sans&family=IBM+Plex+Sans&display=swap" rel="stylesheet">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.3/jquery.min.js" defer></script>
        <script src="../js/script.js" defer> </script>
        <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
        <link rel="manifest" href="/site.webmanifest">
    </head>
    <body class="contentBg">
        <!-- Barra de navegación-->
        <div class="topnav" id="myTopnav">
            <a href="../index.html">Inicio</a>
            <a href="../html/unidad1.html">Unidad 1</a>
            <a href="../html/unidad2.html">Unidad 2</a>
            <a href="../html/unidad3.html">Unidad 3</a>
            <a class="active"> Unidad 4 </a>
            <a href="../html/practicas.html"> Prácticas </a>
            <a href="../html/diseñoEq.html"> Diseño de Equipos</a>
            <a href="../html/about.html"> Acerca de</a>
            <a href="javascript:void(0);" class="icon" onclick="changeTopnav()">
                <i class="fa fa-bars"></i>
            </a>
        </div>
        <!-- Div que separa el indice y el texto-->
        <div class="flex-TextContainer" id="flex">
            <div class="contentIndex" id="contentIndex">
                <div class="banner" id="banner"> 
                    <button class="hide" type="button" onclick="hide()" id="hide">Ocultar</button>
                    <p class="indexTitle" id="title"> Índice </p>
                </div>
                <div id="index">
                    <ol>
                        <li style="color:rgba(0, 0, 0, 0)">&nbsp;</li>
                        <li style="color:rgba(0, 0, 0, 0); margin:0">&nbsp;</li>
                        <li style="color:rgba(0, 0, 0, 0); margin:0">&nbsp;</li>

                        <li><a href="#propal">Procesamiento paralelo</a>
                            <ol>
                                <li><a href="#asba">Aspectos básicos de la computación paralela</a>
                                </li>
                                <li>
                                    <a href="#tipos">Tipos de computación paralela</a>
                                    <ol>
                                        <li><a href="#cla">Clasificación</a></li>
                                        <li><a href="#sec">Arquitectura de computadores secuenciales</a></li>
                                        <li><a href="#memorg">Organización de direcciones de memoria</a></li>
                                    </ol>
                                </li>
                                <li>
                                    <a href="#memcomp">Sistema de memoria compartida</a>
                                    <ol>
                                        <li><a href="#multi">Multiprocesadores</a></li>
                                        <li><a href="#dinamica">Redes de interconexión dinámica</a></li>
                                        <li><a href="#mediocomp">Redes de medio compartida</a></li>
                                        <li><a href="#conmu">Redes conmutadas</a></li>
                                    </ol>
                                </li>
                                <li><a href="#distri">
                                    Sistema de memoria
                                    distribuida
                                    </a>
                                </li>
                                <li>
                                    <a href="#casest">Casos de estudio</a>
                                </li>
                            </ol>
                        </li>
                    </ol>
                </div>
            </div>
            <div class="info" id="info">
                <p class="title" id="propal">
                    Unidad 4: Procesamiento paralelo
                </p>
                <p class="title" id="asba">
                    4.1 Aspectos básicos de la computación paralela
                </p>
                <p class="text">
                    La computación paralela es un enfoque de procesamiento de datos que utiliza múltiples recursos de computación trabajando en conjunto para resolver un problema o realizar una tarea de manera más rápida y eficiente.
                    Algunos aspectos básicos de la computación paralela son
                </p>
                <p class="subtitle">
                    Paralelismo
                </p>
                <p class="text">
                    El paralelismo es el concepto central de la computación paralela. Consiste en dividir una tarea en partes más pequeñas y ejecutarlas simultáneamente en múltiples procesadores o núcleos de procesamiento. Esto permite que varias tareas se realicen al mismo tiempo, lo que puede mejorar significativamente la velocidad de procesamiento.
                </p>
                <p class="subtitle">
                    Tipos de paralelismo
                </p>
                <p class="text">
                    Hay diferentes tipos de paralelismo en la computación paralela, como el paralelismo de datos, el paralelismo de tareas y el paralelismo de instrucciones. El paralelismo de datos implica dividir los datos en pequeñas partes y procesarlas simultáneamente. El paralelismo de tareas implica dividir una tarea en sub-tareas más pequeñas y asignarlas a diferentes procesadores. El paralelismo de instrucciones implica la ejecución simultánea de múltiples instrucciones en diferentes unidades de procesamiento.
                </p>
                <p class="subtitle">
                    Arquitecturas paralelas
                </p>
                <p class="text">
                    Existen diferentes arquitecturas de hardware para la computación paralela, como multiprocesadores, clústeres de computadoras, sistemas de memoria compartida (SMP), sistemas de memoria distribuida (MPP) y sistemas de memoria global (NUMA). Estas arquitecturas están diseñadas para soportar la ejecución simultánea de múltiples hilos o procesos.
                </p>
                <p class="subtitle">
                    Programación paralela
                </p>
                <p class="text">
                    Para aprovechar al máximo la computación paralela, se requiere programación paralela. Esto implica dividir la tarea en sub-tareas que pueden ejecutarse en paralelo y coordinar la comunicación y sincronización entre las diferentes partes del programa. Se utilizan diferentes modelos de programación paralela, como hilos, procesos, directivas de compilador y bibliotecas paralelas.
                </p>
                <p class="subtitle">
                    Beneficios de la computación paralela
                </p>
                <p class="text">
                    La computación paralela ofrece varios beneficios, como una mayor velocidad de procesamiento, capacidad de procesar grandes volúmenes de datos en un tiempo más corto, capacidad de resolver problemas más complejos y capacidad de aprovechar eficientemente los recursos de hardware disponibles.
                </p>
                <p class="title" id="tipos">
                    4.2 Tipos de computación paralela
                </p>
                <p class="text">
                    La computación paralela se refiere a la ejecución simultánea de tareas en múltiples recursos de computación. Hay varios enfoques para lograr la paralelización, como el paralelismo a nivel de bit, instrucción, dato, tarea y memoria. Cada enfoque se utiliza según el tipo de operaciones que se deben realizar y la estructura de los datos. La computación paralela permite mejorar la velocidad y eficiencia en la resolución de problemas al dividir el trabajo entre múltiples procesadores o núcleos de procesamiento.
                </p>
                <p class="subtitle" id="cla">
                    4.2.1 Clasificación
                </p>
                <p class="text">
                    Los tipos de computación paralela se pueden clasificar en función de cómo se organiza y distribuye el trabajo entre los procesadores.Una clasificación general de los tipos de computación paralela puede ser:
                </p>
                <ul>
                    <li><b>Paralelismo a nivel de bit</b>: Se realizan operaciones simultáneas en los bits individuales de los datos. Este tipo de paralelismo se encuentra en procesadores vectoriales y SIMD (Single Instruction, Multiple Data).</li>
                    <li><b>Paralelismo a nivel de instrucción:</b> Se ejecutan múltiples instrucciones simultáneamente en diferentes unidades de procesamiento. Este enfoque se utiliza en procesadores superescalares y en arquitecturas con múltiples núcleos.</li>
                    <li><b>Paralelismo a nivel de dato:</b> Se divide un conjunto de datos en partes más pequeñas y se procesan simultáneamente en múltiples procesadores o núcleos. Cada procesador realiza operaciones en su subconjunto de datos asignado. Es común en aplicaciones de procesamiento de imágenes y análisis de big data.</li>
                    <li><b>Paralelismo a nivel de tarea</b>: Se divide una tarea en sub-tareas más pequeñas y se asignan a diferentes procesadores para su ejecución simultánea. Cada sub-tarea puede ser independiente y realizar una parte específica de la tarea global. Se utiliza en aplicaciones con múltiples tareas que se pueden realizar al mismo tiempo, como la renderización de gráficos en tiempo real.</li>
                    <li><b>Paralelismo a nivel de memoria</b>: Se utilizan múltiples nodos o sistemas con su propia memoria para procesar datos de manera independiente. Cada nodo tiene su propia memoria local y realiza operaciones en su conjunto de datos asignado. La comunicación y la sincronización entre los nodos se realizan a través de redes de interconexión. Este enfoque se encuentra en sistemas de memoria distribuida y sistemas de memoria global.</li>
                </ul>
                <p class="text">
                    Estas son algunas categorías generales de computación paralela, y en la práctica, a menudo se combinan diferentes enfoques para aprovechar al máximo los recursos de hardware disponibles y resolver problemas de manera eficiente. 
                </p>
                <p class="subtitle" id="sec">
                    4.2.2 Arquitectura de computadoras secuenciales
                </p>
                <p class="text">
                    La arquitectura de computadoras secuenciales se refiere a un tipo de diseño de hardware en el que las instrucciones se ejecutan de manera secuencial, es decir, una después de la otra.
                    Algunos aspectos clave de la arquitectura de computadoras secuenciales son:
                </p>
                <ul>
                    <li><b>Unidad de Control (UC)</b>: La UC es responsable de controlar y coordinar las operaciones de la computadora. Se encarga de buscar las instrucciones de la memoria, decodificarlas y enviar señales a las unidades de ejecución correspondientes.</li>
                    <li><b>Unidad Aritmético-Lógica (ALU)</b>: La ALU es responsable de realizar operaciones aritméticas y lógicas, como sumas, restas, multiplicaciones y comparaciones. Es el componente que realiza las operaciones fundamentales en la ejecución de instrucciones.</li>
                    <li><b>Registros</b>: Los registros son ubicaciones de almacenamiento de alta velocidad ubicadas en la CPU. Se utilizan para almacenar datos e instrucciones temporales que se están procesando. Los registros incluyen el contador de programa (PC), que mantiene la dirección de la siguiente instrucción a ejecutar.</li>
                    <li><b>Memoria principal</b>: La memoria principal almacena tanto las instrucciones como los datos que se están procesando. Las instrucciones se almacenan en una parte de la memoria llamada memoria de programa, mientras que los datos se almacenan en otra parte conocida como memoria de datos.</li>
                    <li><b>Bus de datos y bus de control</b>: Estos buses son canales de comunicación utilizados para transferir datos y señales de control entre los diferentes componentes de la computadora. El bus de datos transporta los datos que se deben leer o escribir en la memoria o en los registros, mientras que el bus de control lleva las señales de control que indican las operaciones a realizar.</li>
                    <li><b>Pipeline</b>: Algunas arquitecturas secuenciales pueden utilizar técnicas de pipeline para mejorar el rendimiento. El pipeline divide las instrucciones en etapas y permite que varias instrucciones se ejecuten en paralelo, lo que acelera el procesamiento.</li>
                </ul>
                <p class="subtitle" id="memorg">
                    4.2.3 Organización de direcciones de memoria
                </p>
                <p class="text">
                    La organización de direcciones de memoria se refiere a la forma en que se asignan y gestionan las direcciones de memoria en un sistema de computadora.
                    Algunos aspectos clave de la organización de direcciones de memoria:<br><br>
                    1. <b>Tamaño de palabra</b>: El tamaño de palabra se refiere al número de bits que se pueden leer o escribir en una sola operación de memoria. Puede ser de 8 bits (byte), 16 bits (word), 32 bits (double word) o 64 bits (quad word), entre otros tamaños comunes. El tamaño de palabra influye en la capacidad de direccionamiento y en la cantidad de datos que se pueden acceder simultáneamente.<br><br>
                    2. <b>Espacio de direcciones</b>: El espacio de direcciones es el rango total de direcciones de memoria disponibles en un sistema. Determina la capacidad de almacenamiento que se puede abordar directamente. El tamaño del espacio de direcciones depende de la arquitectura y puede variar entre sistemas. Por ejemplo, en una arquitectura de 32 bits, el espacio de direcciones puede ser de 4 GB.<br><br>
                    3. <b>Direcciones físicas y virtuales</b>: En sistemas con memoria virtual, se utiliza una traducción de direcciones para mapear las direcciones virtuales (utilizadas por los programas) a direcciones físicas (utilizadas por la memoria real). Esto permite una mayor flexibilidad y protección de la memoria, ya que los programas se ejecutan en un espacio de direcciones virtual aislado. <br><br>
                    4. <b>Modos de direccionamiento</b>: Los modos de direccionamiento definen cómo se especifican las direcciones de memoria en las instrucciones del programa. Pueden incluir direccionamiento directo, indirecto, indexado, basado en registros, entre otros. Cada modo tiene su propia forma de calcular y acceder a las direcciones de memoria. <br><br>
                    5. <b>Organización jerárquica</b>: La memoria se organiza en una estructura jerárquica que incluye niveles de caché, memoria principal y almacenamiento secundario (como discos duros). Cada nivel tiene diferentes tiempos de acceso y capacidades de almacenamiento. La jerarquía de memoria se utiliza para optimizar el rendimiento accediendo primero a los niveles de memoria más rápidos y cercanos al procesador. <br><br>
                    6. <b>Alineación de memoria</b>: La alineación de memoria se refiere a la forma en que los datos se almacenan en la memoria. Los datos suelen estar alineados en direcciones de memoria que son múltiplos del tamaño de palabra. La alineación de memoria puede afectar el rendimiento y la eficiencia del acceso a datos. <br><br>
                    La organización de direcciones de memoria es un aspecto fundamental en el diseño de sistemas de computadoras, ya que afecta la capacidad de almacenamiento, la eficiencia del acceso a los datos y la seguridad de la memoria
                </p>
                <p class="title" id="memcomp">
                    4.3 Sistema de memoria compartida
                </p>
                <p class="text">
                    Un sistema de memoria compartida permite a múltiples procesadores o núcleos acceder a un espacio de memoria común, facilitando el intercambio de datos y la programación paralela. Aunque presenta desafíos en términos de sincronización y rendimiento, sigue siendo una arquitectura ampliamente utilizada en sistemas multiprocesador y en entornos de computación paralela.
                </p>
                <p class="subtitle" id="multi">
                    4.3.1 Multiprocesadores
                </p>
                <p class="text">
                    Los sistemas de memoria compartida multiprocesadores (SMP, por sus siglas en inglés: Symmetric Multiprocessing) son arquitecturas de computadoras en las cuales múltiples procesadores o núcleos comparten un espacio de memoria física común. En este tipo de sistemas, todos los procesadores tienen acceso directo a la misma memoria principal, lo que les permite compartir datos y comunicarse de manera eficiente. 
                    En un sistema SMP, cada procesador tiene igualdad de acceso a la memoria y puede ejecutar cualquier tarea. No hay una jerarquía de procesadores maestros o esclavos, y todos pueden realizar tanto operaciones de lectura como de escritura en cualquier dirección de memoria. Esto significa que cualquier procesador puede acceder a cualquier dato en la memoria compartida sin restricciones. Los sistemas SMP son especialmente útiles en entornos donde se requiere un alto rendimiento computacional y una capacidad de procesamiento paralelo eficiente. Estos sistemas pueden escalar agregando más procesadores a la arquitectura, lo que permite realizar tareas intensivas en computación de manera más rápida y eficiente. La ventaja principal de los sistemas SMP es su simplicidad de programación. Los programadores pueden desarrollar aplicaciones paralelas sin tener que preocuparse por la distribución explícita de datos o la sincronización compleja entre procesadores. La memoria compartida proporciona un mecanismo natural para compartir información entre los procesadores, lo que facilita el desarrollo de software paralelo y acelera el tiempo de desarrollo. Sin embargo, los sistemas SMP también tienen desafíos. A medida que se agrega más procesadores, puede haber problemas de latencia y congestión en la memoria compartida, lo que puede afectar el rendimiento global del sistema. Además, la sincronización de acceso a memoria compartida puede introducir cuellos de botella si no se maneja adecuadamente, ya que múltiples procesadores pueden intentar acceder simultáneamente a los mismos datos. Para mitigar estos desafíos, los sistemas SMP a menudo implementan técnicas de caché coherente, donde cada procesador tiene su propia caché para almacenar copias de datos utilizados con frecuencia. Estas cachés se coordinan para mantener la coherencia de los datos compartidos, asegurando que todos los procesadores vean los cambios realizados por otros procesadores en la memoria compartida. 
                </p>
                <p class="subtitle" id="dinamica">
                    4.3.2 Redes de interconexión dinámica
                </p>
                <p class="text">
                    Las redes de interconexión dinámica (DIN, por sus siglas en inglés: Dynamic Interconnection Networks) son un tipo de arquitectura de computadoras utilizada en sistemas multiprocesador y en computación de alto rendimiento. Estas redes permiten la conexión y comunicación eficiente entre los nodos o procesadores del sistema, adaptándose de forma dinámica a las necesidades de procesamiento y a las condiciones de carga de trabajo. En una red de interconexión dinámica, los nodos o procesadores están conectados mediante un conjunto de enlaces o canales de comunicación. La topología de la red puede variar y adaptarse según los requisitos de rendimiento y la configuración del sistema. Algunas topologías comunes incluyen mallas (mesh), árboles (tree), anillos (ring), hipercubos (hypercube), entre otras. La principal característica de las redes de interconexión dinámica es su capacidad para adaptarse y reconfigurarse en tiempo real. Esto significa que la topología de la red puede modificarse durante la ejecución de las aplicaciones o según las necesidades de comunicación de los procesadores. Por ejemplo, se pueden abrir o cerrar enlaces, reconfigurar rutas de comunicación o asignar dinámicamente recursos de red según las demandas del sistema. Esta flexibilidad y adaptabilidad de las redes de interconexión dinámica permiten optimizar el rendimiento y la eficiencia de los sistemas multiprocesador. Al ajustar la topología y la configuración de la red en función de la carga de trabajo, se pueden minimizar las latencias de comunicación, reducir los cuellos de botella y maximizar el rendimiento global del sistema. Las redes de interconexión dinámica también pueden incorporar mecanismos de enrutamiento inteligente y técnicas avanzadas de conmutación para mejorar el flujo de datos y la escalabilidad. Estos mecanismos permiten seleccionar las rutas óptimas y minimizar las interferencias entre los nodos, optimizando así la comunicación entre los procesadores y evitando
                    congestiones o bloqueos en la red. 
                </p>
                <p class="subtitle" id="mediocomp">
                    4.3.3 Redes de medio compartido
                </p>
                <p class="text">
                    El medio compartido se refiere a un recurso de red que es compartido por múltiples dispositivos o usuarios para transmitir datos. En el contexto de las redes de comunicación, el medio compartido puede ser un canal físico, como un cable o una banda de frecuencia, que se utiliza para transmitir señales de datos entre los dispositivos conectados. Cuando varios dispositivos comparten el mismo medio, es necesario establecer un protocolo de acceso al medio para evitar colisiones y garantizar un uso eficiente del recurso.
                    Algunos ejemplos de medios compartidos son: 
                </p>
                <ul>
                    <li><b>Ethernet</b>: En las redes Ethernet, los dispositivos comparten el mismo cable de red para transmitir datos. Se utiliza el protocolo de acceso al medio conocido como CSMA/CD (Carrier Sense Multiple Access with Collision Detection), que permite a los dispositivos escuchar el medio antes de transmitir para evitar colisiones. Si se detecta una colisión, los dispositivos esperan un tiempo aleatorio antes de volver a intentar la transmisión.</li>
                    <li><b>Medios inalámbricos</b>: En las redes inalámbricas, como las redes Wi-Fi, el medio compartido es el espectro de frecuencia utilizado para la transmisión de datos. Varios dispositivos comparten las mismas frecuencias para enviar y recibir señales inalámbricas. Se utilizan protocolos de acceso al medio, como CSMA/CA (Carrier Sense Multiple Access with Collision Avoidance), para evitar colisiones y administrar el acceso equitativo al medio inalámbrico.</li>
                    <li><b>Medios satelitales</b>: En las comunicaciones satelitales, múltiples usuarios comparten el mismo satélite como medio de transmisión. Los datos se transmiten desde los usuarios a través de enlaces ascendentes al satélite, y luego se retransmiten a los usuarios a través de enlaces descendentes. Los sistemas de acceso múltiple por división de tiempo (TDMA) y acceso múltiple por división de frecuencia (FDMA) se utilizan para permitir que varios usuarios compartan el mismo ancho de banda del satélite.</li>
                </ul>
                <p class="subtitle" id="conmu">
                    4.3.4 Redes conmutadas
                </p>
                <p class="text">
                    En un sistema de memoria compartida conmutada, varios nodos o procesadores se conectan a una memoria compartida centralizada a través de un conmutador. Cada nodo tiene acceso directo a la memoria compartida y puede leer o escribir datos en ella. Este tipo de sistema permite que múltiples nodos accedan a los mismos datos en tiempo real, lo que facilita la comunicación y la sincronización entre los procesadores. El conmutador actúa como un punto central de conexión, permitiendo que los nodos se comuniquen entre sí y accedan a la memoria compartida de manera eficiente. El conmutador puede ser un dispositivo de hardware dedicado diseñado específicamente para esta función, o puede estar implementado a nivel de software en un sistema distribuido. Cuando un nodo desea acceder a un área específica de la memoria compartida, envía una solicitud al conmutador. El conmutador se encarga de gestionar el acceso a la memoria y garantizar que no haya conflictos entre las solicitudes de diferentes nodos. Puede utilizar algoritmos de arbitraje para decidir qué nodo tiene prioridad en el acceso a la memoria en caso de conflictos. La ventaja de los sistemas de memoria compartida conmutada es que permiten un acceso rápido y directo a la memoria compartida, lo que facilita la comunicación y la cooperación entre los nodos. Esto es especialmente útil en aplicaciones que requieren un alto grado de interacción entre los procesadores, como cálculos paralelos, simulaciones científicas, bases de datos distribuidas y sistemas de tiempo real. Sin embargo, también existen desafíos en los sistemas de memoria compartida conmutada, como la gestión de la coherencia de la memoria compartida, es decir, garantizar que todos los nodos vean los mismos datos en todo momento. Esto requiere protocolos y mecanismos de sincronización adecuados para mantener la coherencia de los datos en un entorno distribuido y concurrente. 
                </p>
                <p class="title" id="distri">
                    4.4 Sistema de memoria distribuida
                </p>
                <p class="text">
                    Los sistemas de memoria distribuida son aquellos en los que la memoria está distribuida entre varios nodos en una red. Cada nodo tiene su propia memoria local y los datos se almacenan y acceden de manera distribuida. Los nodos se comunican entre sí para compartir datos, lo que permite escalar horizontalmente y aumentar la disponibilidad de recursos. Sin embargo, se requieren algoritmos y protocolos para garantizar la consistencia de los datos y evitar conflictos. Estos sistemas se utilizan en bases de datos distribuidas, sistemas de almacenamiento en la nube y computación en clúster para lograr un mayor rendimiento y escalabilidad en entornos distribuidos.
                </p>
                <p class="subtitle">
                    Multicomputadores
                </p>
                <p class="text">
                    Los multicomputadores, también conocidos como sistemas multicomputador o clústeres, son sistemas compuestos por múltiples computadoras independientes interconectadas que trabajan juntas como una sola entidad. Estas computadoras, también llamadas nodos, se comunican y coordinan sus acciones para realizar tareas de manera paralela y distribuida. En un multicomputador, cada nodo tiene su propia memoria, procesador y sistema operativo, lo que permite un alto grado de independencia y flexibilidad. Los nodos pueden ejecutar aplicaciones por separado o colaborar en la ejecución de tareas más grandes y complejas dividiéndolas en subtareas más pequeñas que se ejecutan de manera concurrente. La comunicación entre los nodos se realiza a través de una red de interconexión, que puede ser tanto local como global. Esta red permite el intercambio de datos y la sincronización de actividades entre los nodos. Algunos ejemplos comunes de redes de interconexión utilizadas en multicomputadores incluyen Ethernet, InfiniBand y Myrinet. Los multicomputadores son ampliamente utilizados en aplicaciones que requieren un alto rendimiento computacional, como la simulación científica, el procesamiento de datos masivos, la renderización de gráficos y la investigación en inteligencia artificial. Al aprovechar la capacidad de cómputo distribuido y paralelo de múltiples nodos, los multicomputadores pueden acelerar significativamente el tiempo de ejecución de tareas complejas al dividirlas en partes más pequeñas y procesarlas simultáneamente. 
                </p>
                <p class="subtitle">
                    Redes de interconexión estática
                </p>
                <p class="text">
                    Las redes de interconexión estática son sistemas de interconexión de dispositivos en los que las conexiones entre los nodos de la red se establecen de manera fija y permanente. En este tipo de redes, la topología de la red y las rutas de comunicación entre los nodos se definen previamente y no cambian durante el funcionamiento normal de la red. En las redes de interconexión estática, cada nodo de la red está conectado directamente a otros nodos específicos según la configuración establecida. Estas conexiones pueden ser cables físicos, enrutadores dedicados o cualquier otro medio de comunicación que permita la transmisión de datos. Una de las ventajas de las redes de interconexión estática es su simplicidad y previsibilidad. Dado que las conexiones son fijas, no es necesario realizar configuraciones dinámicas o establecer rutas en tiempo real, lo que puede simplificar la administración de la red y reducir la complejidad de su funcionamiento. Sin embargo, las redes de interconexión estática también tienen algunas limitaciones. Por un lado, la falta de flexibilidad puede dificultar la adaptación de la red a cambios en la demanda de tráfico o en la disponibilidad de recursos. Además, la presencia de conexiones fijas puede generar cuellos de botella en la red si el tráfico se concentra en ciertos enlaces o nodos específicos. 
                </p>
                <p class="title" id="casest">
                    4.5 Casos de estudio
                </p>
                <p class="text">
                    <b>Nvidia: </b><br><br>
                    NVIDIA es una empresa que diseña y fabrica tarjetas gráficas, procesadores y sistemas de computación de alto rendimiento. La tecnología CUDA de NVIDIA permite la computación paralela en sus tarjetas gráficas, lo que las hace ideales para aplicaciones como la inteligencia artificial, la simulación y el procesamiento de imágenes.<br><br>
                    <b>Amazon Web Services: </b><br><br>
                    Amazon Web Services (AWS) es un servicio de computación en la nube que ofrece una amplia variedad de opciones de computación paralela, incluyendo Amazon EC2, que permite el uso de múltiples instancias para procesamiento paralelo de grandes conjuntos de datos.<br><br>
                    <b>Google Cloud Platform: </b><br><br>
                    Google Cloud Platform es otro servicio de computación en la nube que utiliza la computación paralela para procesar grandes cantidades de datos. Google también ha desarrollado su propio procesador de inteligencia artificial llamado TPU (Tensor Processing Unit), que utiliza la computación paralela para acelerar el entrenamiento de redes neuronales.<br><br>
                    <b>IBM: </b><br><br>
                    IBM ha desarrollado una variedad de productos y servicios de computación paralela, incluyendo el sistema de computación en paralelo IBM Blue Gene, que ha sido utilizado en aplicaciones científicas y de ingeniería, y el servidor de alto rendimiento IBM Power System AC922, que utiliza procesadores IBM POWER9 para ofrecer computación de alto rendimiento.<br><br>
                </p>
            </div>
        </div>
    </body>
</html>